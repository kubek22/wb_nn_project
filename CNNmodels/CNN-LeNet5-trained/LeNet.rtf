{\rtf1\ansi\ansicpg1252\cocoartf2757
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 /Users/katebokhan/anaconda3/envs/6.86x/bin/python /Users/katebokhan/Downloads/DTD-LeNet5/task_1/train.py\
Using mps device\
\
Epoch 1\
----------------------------------------------\
loss: 3.856170  [    0/11280]\
loss: 3.838598  [ 2560/11280]\
loss: 3.833405  [ 5120/11280]\
loss: 3.807267  [ 7680/11280]\
loss: 3.849493  [10240/11280]\
Training,	Loss: 3.829	| Accuracy: 3.121\
Validation,	Loss: 3.765	| Accuracy: 4.628\
\
Epoch 2\
----------------------------------------------\
loss: 3.768546  [    0/11280]\
loss: 3.827403  [ 2560/11280]\
loss: 3.699081  [ 5120/11280]\
loss: 3.748479  [ 7680/11280]\
loss: 3.731404  [10240/11280]\
Training,	Loss: 3.741	| Accuracy: 5.222\
Validation,	Loss: 3.709	| Accuracy: 6.330\
\
Epoch 3\
----------------------------------------------\
loss: 3.645507  [    0/11280]\
loss: 3.728102  [ 2560/11280]\
loss: 3.730284  [ 5120/11280]\
loss: 3.729160  [ 7680/11280]\
loss: 3.678383  [10240/11280]\
Training,	Loss: 3.697	| Accuracy: 6.073\
Validation,	Loss: 3.684	| Accuracy: 6.809\
\
Epoch 4\
----------------------------------------------\
loss: 3.690459  [    0/11280]\
loss: 3.589900  [ 2560/11280]\
loss: 3.628746  [ 5120/11280]\
loss: 3.673104  [ 7680/11280]\
loss: 3.616930  [10240/11280]\
Training,	Loss: 3.663	| Accuracy: 6.729\
Validation,	Loss: 3.627	| Accuracy: 8.511\
\
Epoch 5\
----------------------------------------------\
loss: 3.605218  [    0/11280]\
loss: 3.647510  [ 2560/11280]\
loss: 3.694544  [ 5120/11280]\
loss: 3.680571  [ 7680/11280]\
loss: 3.665899  [10240/11280]\
Training,	Loss: 3.625	| Accuracy: 8.121\
Validation,	Loss: 3.632	| Accuracy: 8.032\
\
Epoch 6\
----------------------------------------------\
loss: 3.663429  [    0/11280]\
loss: 3.590011  [ 2560/11280]\
loss: 3.625708  [ 5120/11280]\
loss: 3.520518  [ 7680/11280]\
loss: 3.643676  [10240/11280]\
Training,	Loss: 3.603	| Accuracy: 8.369\
Validation,	Loss: 3.587	| Accuracy: 9.309\
\
Epoch 7\
----------------------------------------------\
loss: 3.594351  [    0/11280]\
loss: 3.566275  [ 2560/11280]\
loss: 3.575954  [ 5120/11280]\
loss: 3.582869  [ 7680/11280]\
loss: 3.632507  [10240/11280]\
Training,	Loss: 3.575	| Accuracy: 8.936\
Validation,	Loss: 3.547	| Accuracy: 9.947\
\
Epoch 8\
----------------------------------------------\
loss: 3.557748  [    0/11280]\
loss: 3.474277  [ 2560/11280]\
loss: 3.556885  [ 5120/11280]\
loss: 3.525471  [ 7680/11280]\
loss: 3.596805  [10240/11280]\
Training,	Loss: 3.549	| Accuracy: 9.371\
Validation,	Loss: 3.530	| Accuracy: 10.213\
\
Epoch 9\
----------------------------------------------\
loss: 3.554901  [    0/11280]\
loss: 3.577376  [ 2560/11280]\
loss: 3.602312  [ 5120/11280]\
loss: 3.575809  [ 7680/11280]\
loss: 3.461691  [10240/11280]\
Training,	Loss: 3.540	| Accuracy: 9.885\
Validation,	Loss: 3.513	| Accuracy: 10.372\
\
Epoch 10\
----------------------------------------------\
loss: 3.506632  [    0/11280]\
loss: 3.595212  [ 2560/11280]\
loss: 3.614019  [ 5120/11280]\
loss: 3.509695  [ 7680/11280]\
loss: 3.502782  [10240/11280]\
Training,	Loss: 3.509	| Accuracy: 10.310\
Validation,	Loss: 3.487	| Accuracy: 11.968\
\
Epoch 11\
----------------------------------------------\
loss: 3.570872  [    0/11280]\
loss: 3.582438  [ 2560/11280]\
loss: 3.459307  [ 5120/11280]\
loss: 3.612952  [ 7680/11280]\
loss: 3.506244  [10240/11280]\
Training,	Loss: 3.493	| Accuracy: 10.629\
Validation,	Loss: 3.475	| Accuracy: 11.649\
\
Epoch 12\
----------------------------------------------\
loss: 3.415308  [    0/11280]\
loss: 3.524074  [ 2560/11280]\
loss: 3.467514  [ 5120/11280]\
loss: 3.445799  [ 7680/11280]\
loss: 3.471769  [10240/11280]\
Training,	Loss: 3.472	| Accuracy: 11.152\
Validation,	Loss: 3.473	| Accuracy: 11.596\
\
Epoch 13\
----------------------------------------------\
loss: 3.342959  [    0/11280]\
loss: 3.409523  [ 2560/11280]\
loss: 3.350517  [ 5120/11280]\
loss: 3.477786  [ 7680/11280]\
loss: 3.481129  [10240/11280]\
Training,	Loss: 3.454	| Accuracy: 11.711\
Validation,	Loss: 3.467	| Accuracy: 11.436\
\
Epoch 14\
----------------------------------------------\
loss: 3.553199  [    0/11280]\
loss: 3.349598  [ 2560/11280]\
loss: 3.502421  [ 5120/11280]\
loss: 3.448720  [ 7680/11280]\
loss: 3.497089  [10240/11280]\
Training,	Loss: 3.441	| Accuracy: 11.826\
Validation,	Loss: 3.451	| Accuracy: 12.553\
\
Epoch 15\
----------------------------------------------\
loss: 3.362344  [    0/11280]\
loss: 3.584720  [ 2560/11280]\
loss: 3.533207  [ 5120/11280]\
loss: 3.239466  [ 7680/11280]\
loss: 3.381814  [10240/11280]\
Training,	Loss: 3.410	| Accuracy: 12.713\
Validation,	Loss: 3.463	| Accuracy: 11.915\
\
Epoch 16\
----------------------------------------------\
loss: 3.449149  [    0/11280]\
loss: 3.379353  [ 2560/11280]\
loss: 3.372597  [ 5120/11280]\
loss: 3.367559  [ 7680/11280]\
loss: 3.345131  [10240/11280]\
Training,	Loss: 3.408	| Accuracy: 12.846\
Validation,	Loss: 3.419	| Accuracy: 12.287\
\
Epoch 17\
----------------------------------------------\
loss: 3.580222  [    0/11280]\
loss: 3.441022  [ 2560/11280]\
loss: 3.299325  [ 5120/11280]\
loss: 3.331632  [ 7680/11280]\
loss: 3.264371  [10240/11280]\
Training,	Loss: 3.374	| Accuracy: 13.316\
Validation,	Loss: 3.428	| Accuracy: 13.138\
\
Epoch 18\
----------------------------------------------\
loss: 3.434857  [    0/11280]\
loss: 3.552883  [ 2560/11280]\
loss: 3.358203  [ 5120/11280]\
loss: 3.227709  [ 7680/11280]\
loss: 3.270094  [10240/11280]\
Training,	Loss: 3.350	| Accuracy: 13.777\
Validation,	Loss: 3.424	| Accuracy: 12.606\
\
Epoch 19\
----------------------------------------------\
loss: 3.446825  [    0/11280]\
loss: 3.333767  [ 2560/11280]\
loss: 3.361429  [ 5120/11280]\
loss: 3.397558  [ 7680/11280]\
loss: 3.346177  [10240/11280]\
Training,	Loss: 3.347	| Accuracy: 13.839\
Validation,	Loss: 3.399	| Accuracy: 12.660\
\
Epoch 20\
----------------------------------------------\
loss: 3.237084  [    0/11280]\
loss: 3.453669  [ 2560/11280]\
loss: 3.470329  [ 5120/11280]\
loss: 3.268448  [ 7680/11280]\
loss: 3.216229  [10240/11280]\
Training,	Loss: 3.338	| Accuracy: 13.901\
Validation,	Loss: 3.393	| Accuracy: 13.883\
\
Epoch 21\
----------------------------------------------\
loss: 3.268682  [    0/11280]\
loss: 3.420195  [ 2560/11280]\
loss: 3.214004  [ 5120/11280]\
loss: 3.405247  [ 7680/11280]\
loss: 3.539250  [10240/11280]\
Training,	Loss: 3.304	| Accuracy: 14.805\
Validation,	Loss: 3.430	| Accuracy: 12.500\
\
Epoch 22\
----------------------------------------------\
loss: 3.294565  [    0/11280]\
loss: 3.277299  [ 2560/11280]\
loss: 3.183100  [ 5120/11280]\
loss: 3.350661  [ 7680/11280]\
loss: 3.301318  [10240/11280]\
Training,	Loss: 3.292	| Accuracy: 14.539\
Validation,	Loss: 3.343	| Accuracy: 14.415\
\
Epoch 23\
----------------------------------------------\
loss: 3.344792  [    0/11280]\
loss: 3.303019  [ 2560/11280]\
loss: 3.305914  [ 5120/11280]\
loss: 3.305697  [ 7680/11280]\
loss: 3.251004  [10240/11280]\
Training,	Loss: 3.279	| Accuracy: 15.284\
Validation,	Loss: 3.339	| Accuracy: 14.947\
\
Epoch 24\
----------------------------------------------\
loss: 3.208271  [    0/11280]\
loss: 3.333208  [ 2560/11280]\
loss: 3.163717  [ 5120/11280]\
loss: 3.276603  [ 7680/11280]\
loss: 3.248767  [10240/11280]\
Training,	Loss: 3.263	| Accuracy: 15.523\
Validation,	Loss: 3.377	| Accuracy: 13.830\
\
Epoch 25\
----------------------------------------------\
loss: 3.413705  [    0/11280]\
loss: 3.061688  [ 2560/11280]\
loss: 3.264201  [ 5120/11280]\
loss: 3.422254  [ 7680/11280]\
loss: 3.118332  [10240/11280]\
Training,	Loss: 3.254	| Accuracy: 15.798\
Validation,	Loss: 3.376	| Accuracy: 14.149\
\
Epoch 26\
----------------------------------------------\
loss: 3.244743  [    0/11280]\
loss: 3.242789  [ 2560/11280]\
loss: 3.276295  [ 5120/11280]\
loss: 3.138404  [ 7680/11280]\
loss: 3.185738  [10240/11280]\
Training,	Loss: 3.225	| Accuracy: 15.949\
Validation,	Loss: 3.376	| Accuracy: 14.468\
\
Epoch 27\
----------------------------------------------\
loss: 3.195680  [    0/11280]\
loss: 3.109151  [ 2560/11280]\
loss: 3.301347  [ 5120/11280]\
loss: 3.090767  [ 7680/11280]\
loss: 3.081399  [10240/11280]\
Training,	Loss: 3.234	| Accuracy: 16.489\
Validation,	Loss: 3.350	| Accuracy: 14.521\
\
Epoch 28\
----------------------------------------------\
loss: 3.200632  [    0/11280]\
loss: 3.298831  [ 2560/11280]\
loss: 3.120414  [ 5120/11280]\
loss: 3.213407  [ 7680/11280]\
loss: 3.185394  [10240/11280]\
Training,	Loss: 3.221	| Accuracy: 16.321\
Validation,	Loss: 3.340	| Accuracy: 15.213\
\
Epoch 29\
----------------------------------------------\
loss: 3.157532  [    0/11280]\
loss: 3.378596  [ 2560/11280]\
loss: 3.196362  [ 5120/11280]\
loss: 3.145336  [ 7680/11280]\
loss: 3.177790  [10240/11280]\
Training,	Loss: 3.223	| Accuracy: 16.179\
Validation,	Loss: 3.361	| Accuracy: 14.628\
\
Epoch 30\
----------------------------------------------\
loss: 3.196554  [    0/11280]\
loss: 3.269735  [ 2560/11280]\
loss: 3.305748  [ 5120/11280]\
loss: 3.157510  [ 7680/11280]\
loss: 3.220728  [10240/11280]\
Training,	Loss: 3.189	| Accuracy: 16.853\
Validation,	Loss: 3.373	| Accuracy: 13.989\
\
Epoch 31\
----------------------------------------------\
loss: 3.121911  [    0/11280]\
loss: 3.162580  [ 2560/11280]\
loss: 3.146617  [ 5120/11280]\
loss: 3.246598  [ 7680/11280]\
loss: 3.334160  [10240/11280]\
Training,	Loss: 3.179	| Accuracy: 17.030\
Validation,	Loss: 3.340	| Accuracy: 14.628\
\
Epoch 32\
----------------------------------------------\
loss: 3.116668  [    0/11280]\
loss: 3.116416  [ 2560/11280]\
loss: 3.110673  [ 5120/11280]\
loss: 3.215396  [ 7680/11280]\
loss: 3.187155  [10240/11280]\
Training,	Loss: 3.166	| Accuracy: 17.190\
Validation,	Loss: 3.339	| Accuracy: 15.213\
\
Epoch 33\
----------------------------------------------\
loss: 2.933701  [    0/11280]\
loss: 3.132187  [ 2560/11280]\
loss: 2.976741  [ 5120/11280]\
loss: 3.388241  [ 7680/11280]\
loss: 3.166772  [10240/11280]\
Training,	Loss: 3.163	| Accuracy: 17.101\
Validation,	Loss: 3.347	| Accuracy: 14.362\
\
Epoch 34\
----------------------------------------------\
loss: 3.052186  [    0/11280]\
loss: 3.196997  [ 2560/11280]\
loss: 3.186448  [ 5120/11280]\
loss: 3.264369  [ 7680/11280]\
loss: 3.092744  [10240/11280]\
Training,	Loss: 3.159	| Accuracy: 17.207\
Validation,	Loss: 3.280	| Accuracy: 15.745\
\
Epoch 35\
----------------------------------------------\
loss: 3.151249  [    0/11280]\
loss: 3.209533  [ 2560/11280]\
loss: 3.079384  [ 5120/11280]\
loss: 3.214978  [ 7680/11280]\
loss: 3.050309  [10240/11280]\
Training,	Loss: 3.142	| Accuracy: 18.014\
Validation,	Loss: 3.296	| Accuracy: 16.223\
\
Epoch 36\
----------------------------------------------\
loss: 3.124630  [    0/11280]\
loss: 3.294955  [ 2560/11280]\
loss: 3.270485  [ 5120/11280]\
loss: 3.106428  [ 7680/11280]\
loss: 3.046598  [10240/11280]\
Training,	Loss: 3.148	| Accuracy: 17.500\
Validation,	Loss: 3.318	| Accuracy: 15.479\
\
Epoch 37\
----------------------------------------------\
loss: 3.155417  [    0/11280]\
loss: 3.123788  [ 2560/11280]\
loss: 2.960790  [ 5120/11280]\
loss: 3.088675  [ 7680/11280]\
loss: 3.236088  [10240/11280]\
Training,	Loss: 3.132	| Accuracy: 18.271\
Validation,	Loss: 3.305	| Accuracy: 15.957\
\
Epoch 38\
----------------------------------------------\
loss: 2.922395  [    0/11280]\
loss: 3.134524  [ 2560/11280]\
loss: 3.258052  [ 5120/11280]\
loss: 3.199906  [ 7680/11280]\
loss: 2.945903  [10240/11280]\
Training,	Loss: 3.127	| Accuracy: 18.369\
Validation,	Loss: 3.322	| Accuracy: 14.947\
\
Epoch 39\
----------------------------------------------\
loss: 3.100137  [    0/11280]\
loss: 3.106793  [ 2560/11280]\
loss: 3.234631  [ 5120/11280]\
loss: 3.060526  [ 7680/11280]\
loss: 3.104752  [10240/11280]\
Training,	Loss: 3.096	| Accuracy: 18.723\
Validation,	Loss: 3.273	| Accuracy: 15.957\
\
Epoch 40\
----------------------------------------------\
loss: 3.039948  [    0/11280]\
loss: 3.231387  [ 2560/11280]\
loss: 3.020737  [ 5120/11280]\
loss: 3.218577  [ 7680/11280]\
loss: 3.171794  [10240/11280]\
Training,	Loss: 3.098	| Accuracy: 18.307\
Validation,	Loss: 3.323	| Accuracy: 16.489\
\
Epoch 41\
----------------------------------------------\
loss: 3.155338  [    0/11280]\
loss: 2.925363  [ 2560/11280]\
loss: 3.073412  [ 5120/11280]\
loss: 3.071063  [ 7680/11280]\
loss: 3.117467  [10240/11280]\
Training,	Loss: 3.095	| Accuracy: 18.679\
Validation,	Loss: 3.289	| Accuracy: 15.532\
\
Epoch 42\
----------------------------------------------\
loss: 3.063804  [    0/11280]\
loss: 3.124013  [ 2560/11280]\
loss: 3.234286  [ 5120/11280]\
loss: 2.893454  [ 7680/11280]\
loss: 3.100750  [10240/11280]\
Training,	Loss: 3.084	| Accuracy: 18.972\
Validation,	Loss: 3.305	| Accuracy: 16.223\
\
Epoch 43\
----------------------------------------------\
loss: 3.049385  [    0/11280]\
loss: 2.962225  [ 2560/11280]\
loss: 3.156328  [ 5120/11280]\
loss: 2.968846  [ 7680/11280]\
loss: 3.069512  [10240/11280]\
Training,	Loss: 3.072	| Accuracy: 19.309\
Validation,	Loss: 3.267	| Accuracy: 16.223\
\
Epoch 44\
----------------------------------------------\
loss: 3.083160  [    0/11280]\
loss: 3.070019  [ 2560/11280]\
loss: 3.021340  [ 5120/11280]\
loss: 3.043478  [ 7680/11280]\
loss: 3.072148  [10240/11280]\
Training,	Loss: 3.080	| Accuracy: 19.167\
Validation,	Loss: 3.333	| Accuracy: 15.266\
\
Epoch 45\
----------------------------------------------\
loss: 2.943687  [    0/11280]\
loss: 3.160603  [ 2560/11280]\
loss: 3.219309  [ 5120/11280]\
loss: 2.998110  [ 7680/11280]\
loss: 3.007924  [10240/11280]\
Training,	Loss: 3.059	| Accuracy: 19.867\
Validation,	Loss: 3.298	| Accuracy: 14.787\
\
Epoch 46\
----------------------------------------------\
loss: 3.091497  [    0/11280]\
loss: 3.188293  [ 2560/11280]\
loss: 3.235353  [ 5120/11280]\
loss: 3.211421  [ 7680/11280]\
loss: 3.140589  [10240/11280]\
Training,	Loss: 3.063	| Accuracy: 19.158\
Validation,	Loss: 3.337	| Accuracy: 15.213\
\
Epoch 47\
----------------------------------------------\
loss: 3.003649  [    0/11280]\
loss: 2.974247  [ 2560/11280]\
loss: 2.910989  [ 5120/11280]\
loss: 3.165998  [ 7680/11280]\
loss: 3.060284  [10240/11280]\
Training,	Loss: 3.055	| Accuracy: 19.548\
Validation,	Loss: 3.320	| Accuracy: 16.543\
\
Epoch 48\
----------------------------------------------\
loss: 2.917682  [    0/11280]\
loss: 3.163321  [ 2560/11280]\
loss: 3.073243  [ 5120/11280]\
loss: 3.244930  [ 7680/11280]\
loss: 2.899729  [10240/11280]\
Training,	Loss: 3.035	| Accuracy: 19.849\
Validation,	Loss: 3.320	| Accuracy: 17.074\
\
Epoch 49\
----------------------------------------------\
loss: 2.958687  [    0/11280]\
loss: 3.038624  [ 2560/11280]\
loss: 3.018423  [ 5120/11280]\
loss: 3.057582  [ 7680/11280]\
loss: 2.984346  [10240/11280]\
Training,	Loss: 3.039	| Accuracy: 20.071\
Validation,	Loss: 3.293	| Accuracy: 17.287\
\
Epoch 50\
----------------------------------------------\
loss: 3.153963  [    0/11280]\
loss: 3.113952  [ 2560/11280]\
loss: 2.876838  [ 5120/11280]\
loss: 3.070824  [ 7680/11280]\
loss: 3.112129  [10240/11280]\
Training,	Loss: 3.022	| Accuracy: 20.151\
Validation,	Loss: 3.279	| Accuracy: 16.862\
Time usage: 4:54:51\
Finished Training\
Traceback (most recent call last):\
  File "/Users/katebokhan/Downloads/DTD-LeNet5/task_1/train.py", line 89, in <module>\
    kernels, feature_maps, input_img = util.visualize_kernels_and_feature_maps()\
  File "/Users/katebokhan/Downloads/DTD-LeNet5/task_1/util.py", line 167, in visualize_kernels_and_feature_maps\
    model.load_state_dict(torch.load(path))\
  File "/Users/katebokhan/anaconda3/envs/6.86x/lib/python3.8/site-packages/torch/serialization.py", line 809, in load\
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\
  File "/Users/katebokhan/anaconda3/envs/6.86x/lib/python3.8/site-packages/torch/serialization.py", line 1172, in _load\
    result = unpickler.load()\
  File "/Users/katebokhan/anaconda3/envs/6.86x/lib/python3.8/site-packages/torch/serialization.py", line 1142, in persistent_load\
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\
  File "/Users/katebokhan/anaconda3/envs/6.86x/lib/python3.8/site-packages/torch/serialization.py", line 1116, in load_tensor\
    wrap_storage=restore_location(storage, location),\
  File "/Users/katebokhan/anaconda3/envs/6.86x/lib/python3.8/site-packages/torch/serialization.py", line 217, in default_restore_location\
    result = fn(storage, location)\
  File "/Users/katebokhan/anaconda3/envs/6.86x/lib/python3.8/site-packages/torch/serialization.py", line 182, in _cuda_deserialize\
    device = validate_cuda_device(location)\
  File "/Users/katebokhan/anaconda3/envs/6.86x/lib/python3.8/site-packages/torch/serialization.py", line 166, in validate_cuda_device\
    raise RuntimeError('Attempting to deserialize object on a CUDA '\
RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\
Using mps device\
\
Process finished with exit code 1\
}