{
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 8461855,
     "sourceType": "datasetVersion",
     "datasetId": 5044311
    },
    {
     "sourceId": 8572157,
     "sourceType": "datasetVersion",
     "datasetId": 5125726
    },
    {
     "sourceId": 54224,
     "sourceType": "modelInstanceVersion",
     "modelInstanceId": 45451
    }
   ],
   "dockerImageVersionId": 30716,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "import torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets\n\nclass RSSCN7_DataLoader:\n    def __init__(self, data_dir, batch_size=32, shuffle=False):\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n        self.transform = transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        self.dataset = datasets.ImageFolder(root=self.data_dir, transform=self.transform)\n        self.train_dataset, self.test_dataset = self.split_dataset()\n\n    def split_dataset(self):\n        train_size = int(0.8 * len(self.dataset))\n        test_size = len(self.dataset) - train_size\n\n        train_dataset, test_dataset = random_split(self.dataset, [train_size, test_size])\n        return train_dataset, test_dataset\n\n    def get_train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=self.shuffle)\n\n    def get_test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.status.busy": "2024-06-01T14:35:50.819505Z",
     "iopub.execute_input": "2024-06-01T14:35:50.819889Z",
     "iopub.status.idle": "2024-06-01T14:35:50.829770Z",
     "shell.execute_reply.started": "2024-06-01T14:35:50.819856Z",
     "shell.execute_reply": "2024-06-01T14:35:50.828840Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from torch import nn\nimport torchvision.models as models\n\nclass ResNet18(nn.Module):\n    def __init__(self, num_classes=47):\n        super(ResNet18, self).__init__()\n        self.resnet18 = models.resnet18(pretrained=False)\n        self.resnet18.fc = nn.Linear(self.resnet18.fc.in_features, num_classes)\n\n    def forward(self, x):\n        return self.resnet18(x)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-01T14:35:54.893611Z",
     "iopub.execute_input": "2024-06-01T14:35:54.894004Z",
     "iopub.status.idle": "2024-06-01T14:35:54.900913Z",
     "shell.execute_reply.started": "2024-06-01T14:35:54.893974Z",
     "shell.execute_reply": "2024-06-01T14:35:54.899843Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport numpy as np\nimport copy\nfrom torch import nn\n\ndef k_nearest_metric(k, model, data_loader, device):\n    components = list(model.children())\n    truncated_model = nn.Sequential(*components[:-1])\n    \n    if len(components) == 1:\n        components = list(components[0].children())\n        truncated_model = nn.Sequential(*list(components)[:-1])\n        \n    truncated_model = truncated_model.to(device)\n        \n    tensors = []\n    truncated_model.eval()\n    with torch.no_grad():\n        x = data_loader.dataset[0][0]\n        x = x.unsqueeze(0)\n        x = x.to(device)\n        tensor = truncated_model(x)\n        shape = tensor.shape\n        \n        labels = np.array([], dtype='int64')\n        for x, y in data_loader:\n            labels = np.append(labels, y.cpu().numpy()) \n            x, y = x.to(device), y.to(device)\n    \n            tensor = truncated_model(x)\n            tensors.append(tensor)\n\n    if len(tensors) > 1 and tensors[-1].shape != tensors[0].shape:\n        last_tensor = tensors[-1]\n        tensors = tensors[:-1]\n    else:\n        last_tensor = None\n        \n    tensors = torch.stack(tensors)\n    tensor_length = shape.numel()\n    tensors_shape = torch.Size([tensors.shape[0] * tensors.shape[1], tensor_length])\n    tensors = tensors.view(tensors_shape)\n    \n    if last_tensor is not None:\n        last_tensor_shape = torch.Size([last_tensor.shape[0], tensor_length])\n        last_tensor = last_tensor.view(last_tensor_shape)\n        tensors = torch.cat([tensors, last_tensor], dim=0)\n\n    inf_tensor = torch.full(torch.Size([tensor_length]), float('inf'))\n    \n    metric_results = []\n    for i in range(len(tensors)):\n        label = labels[i]\n        tensor = tensors[i]\n        other_tensors = copy.deepcopy(tensors)\n        other_tensors[i] = inf_tensor\n        distances = torch.norm(tensor - other_tensors, dim=1)\n        \n        k_nearest_indices = torch.topk(distances, k, largest=False).indices\n        k_nearest_indices = k_nearest_indices.tolist()\n\n        matching_labels = np.sum(labels[k_nearest_indices] == label)\n        res = matching_labels / k\n        metric_results.append(res)\n    return np.array(metric_results)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-01T14:36:00.608206Z",
     "iopub.execute_input": "2024-06-01T14:36:00.608937Z",
     "iopub.status.idle": "2024-06-01T14:36:00.625369Z",
     "shell.execute_reply.started": "2024-06-01T14:36:00.608895Z",
     "shell.execute_reply": "2024-06-01T14:36:00.624296Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "ImageNet, self paced",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from torchvision.models import resnet18\n# from ..Resnet18.task.model import ResNet18\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.optim as optim\nimport time\nimport random\n\nknn_metric_train = []\nknn_metric_test = []\nacc_train = []\nacc_test = []\nloss_train =[]\nloss_test = []\ntime_epoch = []\ncur_lambda = []\ncur_learning_rate = []\n\nrandom.seed(42)\n\ntime0 = time.time()\n\ndata_dir = '/kaggle/input/rssnc7/RSSCN7'\nbatch_size = 32\nlearning_rate = 0.0001\nnum_epochs = 100\nlambda_beginning = 0.1\nlambda_end = 1\n\nrsscn7_data_loader = RSSCN7_DataLoader(data_dir, batch_size=batch_size, shuffle=True)\ntrain_loader = rsscn7_data_loader.get_train_dataloader()\ntest_loader = rsscn7_data_loader.get_test_dataloader()\n\nmodel = resnet18(weights='ResNet18_Weights.DEFAULT')\nnum_filters = model.fc.in_features\nmodel.fc = nn.Linear(num_filters, 7)\n\n######### In case of model pretrained on DTD: uploading the weights #################################################\n\n# pretrained_model_path = \"/kaggle/input/resnet18-pretrained-on-dtd/pytorch/version1/1/resnet18_trained_on_DTD_from_80_to_90.pth\"\n# pretrained_resnet18 = ResNet18()\n# pretrained_resnet18.load_state_dict(torch.load(pretrained_model_path, map_location=torch.device(device)))\n\n# model = pretrained_resnet18.to(device)\n\n# model.fc = nn.Linear(47, 7)\n\ncriterion = nn.CrossEntropyLoss()\nopitmizer = optim.Adam(model.parameters(), lr=learning_rate)\nmy_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n\nstep = 0.05\n\ndef train_model_self_paced(model, train_loader, test_loader, criterion, optimizer, num_epochs, learning_rate):\n    device = my_device\n    model.to(device)\n    counter = 0\n\n    lambda_current = lambda_beginning\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0.0\n        correct = 0\n        total = 0\n        train_samples = []\n\n        if lambda_current < 1:\n            with torch.no_grad():\n                for inputs, labels in train_loader:\n                    inputs, labels = inputs.to(device), labels.to(device)\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n                    train_samples.append((inputs, labels, loss.item()))\n\n            train_samples.sort(key=lambda x: x[2])  # sort by loss (the first are the easiest)\n\n            num_samples_current = int(lambda_current * len(train_samples))\n\n            easy_enough_samples = train_samples[:num_samples_current]\n            easy_enough_inputs = torch.cat([x[0] for x in easy_enough_samples])\n            easy_enough_labels = torch.cat([x[1] for x in easy_enough_samples])\n            easy_enough_dataset = TensorDataset(easy_enough_inputs, easy_enough_labels)\n            easy_enough_loader = DataLoader(easy_enough_dataset, batch_size=batch_size, shuffle=True)\n        else:\n            easy_enough_loader = train_loader\n\n        for inputs, labels in easy_enough_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item() * inputs.size(0)\n\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n        knn_metric = np.mean(k_nearest_metric(5, model, easy_enough_loader, device))\n        train_loss = total_loss / len(easy_enough_loader.dataset)\n        train_accuracy = correct / total\n        num_images = len(easy_enough_loader.dataset)\n\n        if train_accuracy >= 0.88:\n            learning_rate = 0.00001\n        \n        if train_accuracy >= 0.93:\n            learning_rate = 0.000001\n            \n        if train_accuracy >= 0.96:\n            learning_rate = 0.0000001\n\n        cur_time_ = time.time() - time0\n\n        print(\"learing_rate = \", learning_rate)\n\n        print(\n            f'Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Images: {num_images}, Lambda: {lambda_current:.2f}, Time: {cur_time_:.2f} seconds')\n        print(f'KNN(k=5) metric train: {knn_metric:.4f}')\n        if train_accuracy > 0.85:\n            if lambda_current < 0.6:\n                lambda_current += step\n                if lambda_current>1:\n                    lambda_current = 1\n            else:\n                counter = counter + 1\n                if counter % 2 == 0:\n                    lambda_current += step\n                    counter = 0\n                    if lambda_current>1:\n                        lambda_current = 1\n                        \n        knn_metric_train.append(knn_metric)\n        acc_train.append(train_accuracy)\n        loss_train.append(train_loss)\n        time_epoch.append(cur_time_)\n        cur_lambda.append(lambda_current)\n        cur_learning_rate.append(learning_rate)\n\n        evaluate_model(model, test_loader, criterion)\n\n    print('Finished Training Successfully')\n\n\ndef evaluate_model(model, test_loader, criterion):\n    model.eval()\n    device = next(model.parameters()).device\n    total_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item() * inputs.size(0)\n\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    test_loss = total_loss / len(test_loader.dataset)\n    test_accuracy = correct / total\n    knn_metric = np.mean(k_nearest_metric(5, model, test_loader, device))\n\n    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n    print(f'KNN(k=5) metric test: {knn_metric:.4f}')\n\n    knn_metric_test.append(knn_metric)\n    acc_test.append(test_accuracy)\n    loss_test.append(test_loss)\n    \n\ntrain_model_self_paced(model, train_loader, test_loader, criterion, opitmizer, num_epochs, learning_rate)\nprint(\"KNN train:\")\nprint(knn_metric_train)\nprint(\"KNN test:\")\nprint(knn_metric_test)\nprint(\"Accuracy train:\")\nprint(acc_train)\nprint(\"Accuracy test:\")\nprint(acc_test)\nprint(\"Loss train:\")\nprint(loss_train)\nprint(\"Loss test:\")\nprint(loss_test)\nprint(\"Time epoch:\")\nprint(time_epoch)\nprint(\"Learning rate:\")\nprint(cur_learning_rate)\nprint(\"Lambdas:\")\nprint(cur_lambda)\n\ntorch.save(model, 'resnet18_imagenet_self_paced.pth')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-01T10:25:10.067905Z",
     "iopub.execute_input": "2024-06-01T10:25:10.068310Z",
     "iopub.status.idle": "2024-06-01T11:30:01.648304Z",
     "shell.execute_reply.started": "2024-06-01T10:25:10.068283Z",
     "shell.execute_reply": "2024-06-01T11:30:01.647186Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "DTD, self paced",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from torchvision.models import resnet18\n# from ..Resnet18.task.model import ResNet18\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nimport torch.nn as nn\nimport torch.optim as optim\nimport time\nimport random\n\nknn_metric_train = []\nknn_metric_test = []\nacc_train = []\nacc_test = []\nloss_train =[]\nloss_test = []\ntime_epoch = []\ncur_lambda = []\ncur_learning_rate = []\n\nrandom.seed(42)\n\ntime0 = time.time()\n\ndata_dir = '/kaggle/input/rssnc7/RSSCN7'\nbatch_size = 32\nlearning_rate = 0.0001\nnum_epochs = 100\nlambda_beginning = 0.1\nlambda_end = 1\n\nrsscn7_data_loader = RSSCN7_DataLoader(data_dir, batch_size=batch_size, shuffle=True)\ntrain_loader = rsscn7_data_loader.get_train_dataloader()\ntest_loader = rsscn7_data_loader.get_test_dataloader()\n\nmy_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\npretrained_model_path = \"/kaggle/input/resnet18-pretrained-on-dtd/pytorch/version1/1/resnet18_trained_on_DTD_from_80_to_90.pth\"\npretrained_resnet18 = ResNet18()\npretrained_resnet18.load_state_dict(torch.load(pretrained_model_path, map_location=torch.device(my_device)))\n\nmodel = pretrained_resnet18.to(my_device)\n\nmodel.fc = nn.Linear(47, 7)\n\ncriterion = nn.CrossEntropyLoss()\nopitmizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nstep = 0.05\n\ndef train_model_self_paced(model, train_loader, test_loader, criterion, optimizer, num_epochs, learning_rate):\n    device = my_device\n    model.to(device)\n    counter = 0\n\n    lambda_current = lambda_beginning\n\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0.0\n        correct = 0\n        total = 0\n        train_samples = []\n\n        if lambda_current < 1:\n            with torch.no_grad():\n                for inputs, labels in train_loader:\n                    inputs, labels = inputs.to(device), labels.to(device)\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n                    train_samples.append((inputs, labels, loss.item()))\n\n            train_samples.sort(key=lambda x: x[2])  # sort by loss (the first are the easiest)\n\n            num_samples_current = int(lambda_current * len(train_samples))\n\n            easy_enough_samples = train_samples[:num_samples_current]\n            easy_enough_inputs = torch.cat([x[0] for x in easy_enough_samples])\n            easy_enough_labels = torch.cat([x[1] for x in easy_enough_samples])\n            easy_enough_dataset = TensorDataset(easy_enough_inputs, easy_enough_labels)\n            easy_enough_loader = DataLoader(easy_enough_dataset, batch_size=batch_size, shuffle=True)\n        else:\n            easy_enough_loader = train_loader\n\n        for inputs, labels in easy_enough_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item() * inputs.size(0)\n\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        knn_metric = np.mean(k_nearest_metric(5, model, easy_enough_loader, device))\n        train_loss = total_loss / len(easy_enough_loader.dataset)\n        train_accuracy = correct / total\n        num_images = len(easy_enough_loader.dataset)\n\n        if train_accuracy >= 0.88:\n            learning_rate = 0.00001\n        \n        if train_accuracy >= 0.93:\n            learning_rate = 0.000001\n            \n        if train_accuracy >= 0.96:\n            learning_rate = 0.0000001\n\n        cur_time_ = time.time() - time0\n\n        print(\"learing_rate = \", learning_rate)\n\n        print(\n            f'Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Images: {num_images}, Lambda: {lambda_current:.2f}, Time: {cur_time_:.2f} seconds')\n        print(f'KNN(k=5) metric train: {knn_metric:.4f}')\n        if train_accuracy > 0.8:\n            if lambda_current < 0.6:\n                lambda_current += step\n                if lambda_current>1:\n                    lambda_current = 1\n            else:\n                counter = counter + 1\n                if counter % 2 == 0:\n                    lambda_current += step\n                    counter = 0\n                    if lambda_current>1:\n                        lambda_current = 1\n\n        knn_metric_train.append(knn_metric)\n        acc_train.append(train_accuracy)\n        loss_train.append(train_loss)\n        time_epoch.append(cur_time_)\n        cur_lambda.append(lambda_current)\n        cur_learning_rate.append(learning_rate)\n\n        evaluate_model(model, test_loader, criterion)\n\n    print('Finished Training Successfully')\n\n\ndef evaluate_model(model, test_loader, criterion):\n    model.eval()\n    device = next(model.parameters()).device\n    total_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item() * inputs.size(0)\n\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    knn_metric = np.mean(k_nearest_metric(5, model, test_loader, device))\n    test_loss = total_loss / len(test_loader.dataset)\n    test_accuracy = correct / total\n\n    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n    print(f'KNN(k=5) metric test: {knn_metric:.4f}')\n\n    knn_metric_test.append(knn_metric)\n    acc_test.append(test_accuracy)\n    loss_test.append(test_loss)\n\n\ntrain_model_self_paced(model, train_loader, test_loader, criterion, opitmizer, num_epochs, learning_rate)\nprint(\"KNN train:\")\nprint(knn_metric_train)\nprint(\"KNN test:\")\nprint(knn_metric_test)\nprint(\"Accuracy train:\")\nprint(acc_train)\nprint(\"Accuracy test:\")\nprint(acc_test)\nprint(\"Loss train:\")\nprint(loss_train)\nprint(\"Loss test:\")\nprint(loss_test)\nprint(\"Time epoch:\")\nprint(time_epoch)\nprint(\"Learning rate:\")\nprint(cur_learning_rate)\nprint(\"Lambdas:\")\nprint(cur_lambda)\n\ntorch.save(model, 'resnet18_DTD_self_paced.pth')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-01T11:30:01.651425Z",
     "iopub.execute_input": "2024-06-01T11:30:01.651743Z",
     "iopub.status.idle": "2024-06-01T12:29:39.469634Z",
     "shell.execute_reply.started": "2024-06-01T11:30:01.651716Z",
     "shell.execute_reply": "2024-06-01T12:29:39.468673Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Transfer_learning_imagenet",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader, random_split\nimport torch.nn as nn\nimport random\nimport time\n\ntime0 = time.time()\n\nrandom.seed(10)\n\nknn_metric_train = []\nknn_metric_test = []\ntrain_acc = []\ntest_acc = []\nloss_train = []\nloss_test = []\nepoch_time = []\nlearning_rates = []\n\nclass RCCN7DataLoader:\n    def __init__(self, data_dir, batch_size=32, shuffle=True):\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n        self.transform = transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        self.dataset = datasets.ImageFolder(root=self.data_dir, transform=self.transform)\n        self.train_dataset, self.test_dataset = self.split_dataset()\n\n    def split_dataset(self):\n        train_size = int(0.8 * len(self.dataset))\n        test_size = len(self.dataset) - train_size\n\n        train_dataset, test_dataset = random_split(self.dataset, [train_size, test_size])\n        return train_dataset, test_dataset\n\n    def get_train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=self.shuffle)\n\n    def get_test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n\ndata_dir = '/kaggle/input/rssnc7/RSSCN7'\nbatch_size = 32\nlearning_rate = 0.001\n\ndata_loader = RCCN7DataLoader(data_dir=data_dir, batch_size=batch_size, shuffle=True)\n\nresnet18 = models.resnet18(weights='ResNet18_Weights.DEFAULT')\n\n\nfor name, param in resnet18.named_parameters():\n    if 'fc' not in name:\n        param.requires_grad = False\n\nnum_classes = 7\n\nresnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n\ndevice = \"cuda\"\nresnet18 = resnet18.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(resnet18.parameters(), lr=learning_rate)\n\n#most optimal\nepochs = 100\n\ndef train_model(model, criterion, optimizer, train_loader, test_loader, num_epochs, learning_rate):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct_predictions = 0\n        total_samples = 0\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            correct_predictions += (predicted == labels).sum().item()\n            total_samples += labels.size(0)\n            \n        knn_metric = np.mean(k_nearest_metric(5, model, train_loader, device))\n        epoch_loss = running_loss / total_samples\n        epoch_accuracy = correct_predictions / total_samples\n\n        if epoch_accuracy >= 85:\n            learning_rate = 0.0005\n\n        if epoch_accuracy >= 89:\n            learning_rate = 0.00005\n\n        if epoch_accuracy >= 92:\n            learning_rate = 0.000001\n\n        time_cur = time.time() - time0\n        print(\"learing_rate = \", learning_rate)\n\n        print(f'Training - Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n        print(f'KNN(k=5) metric train: {knn_metric:.4f}')\n        test_loss, test_accuracy, test_knn = evaluate_model(model, criterion, test_loader)\n        print(f'Testing - Epoch {epoch+1}/{num_epochs}, Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}')\n        print(f'KNN(k=5) metric test: {test_knn:.4f}')\n\n        print(f'Time: {time_cur:.2f} seconds')\n\n        knn_metric_test.append(test_knn)\n        knn_metric_train.append(knn_metric)\n        train_acc.append(epoch_accuracy)\n        test_acc.append(test_accuracy)\n        loss_train.append(epoch_loss)\n        loss_test.append(test_loss)\n        learning_rates.append(learning_rate)\n        epoch_time.append(time_cur)\n\n    print('Training complete.')\n\ndef evaluate_model(model, criterion, test_loader):\n    model.eval()\n    running_loss = 0.0\n    correct_predictions = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            correct_predictions += (predicted == labels).sum().item()\n            total_samples += labels.size(0)\n\n    knn_metric = np.mean(k_nearest_metric(5, model, test_loader, device))\n    test_loss = running_loss / total_samples\n    test_accuracy = correct_predictions / total_samples\n\n    return test_loss, test_accuracy, knn_metric\n\ntrain_loader = data_loader.get_train_dataloader()\ntest_loader = data_loader.get_test_dataloader()\n\ntrain_model(resnet18, criterion, optimizer, train_loader, test_loader, epochs, learning_rate)\n\nprint('Training completed successfully.')\nprint(\"KNN train:\")\nprint(knn_metric_train)\nprint(\"KNN test:\")\nprint(knn_metric_test)\nprint(\"Training accuracy:\")\nprint(train_acc)\nprint(\"Test accuracy:\")\nprint(test_acc)\nprint('Loss train:')\nprint(loss_train)\nprint(\"Test loss:\")\nprint(loss_test)\nprint(\"Learning rate:\")\nprint(learning_rates)\nprint(\"Epoch times:\")\nprint(epoch_time)\n\ntorch.save(resnet18, 'resnet18_imagenet_transfer_learning.pth')",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-01T12:33:42.705714Z",
     "iopub.execute_input": "2024-06-01T12:33:42.706140Z",
     "iopub.status.idle": "2024-06-01T13:36:52.762711Z",
     "shell.execute_reply.started": "2024-06-01T12:33:42.706110Z",
     "shell.execute_reply": "2024-06-01T13:36:52.761611Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "raw",
   "source": "Transfer Learning DTD",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nimport random\nimport time\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets\n\nclass RSSCN7_DataLoader:\n    def __init__(self, data_dir, batch_size=32, shuffle=False):\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n        self.transform = transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n        self.dataset = datasets.ImageFolder(root=self.data_dir, transform=self.transform)\n        self.train_dataset, self.test_dataset = self.split_dataset()\n\n    def split_dataset(self):\n        train_size = int(0.8 * len(self.dataset))\n        test_size = len(self.dataset) - train_size\n\n        train_dataset, test_dataset = random_split(self.dataset, [train_size, test_size])\n        return train_dataset, test_dataset\n\n    def get_train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=self.shuffle)\n\n    def get_test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n\ntime0 = time.time()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nrandom.seed(10)\n\nknn_metric_train = []\nknn_metric_test = []\ntrain_acc = []\ntest_acc = []\nloss_train = []\nloss_test = []\nepoch_time = []\nlearning_rates = []\n\ndata_dir = '/kaggle/input/rssnc7/RSSCN7'\nbatch_size = 32\nlearning_rate = 0.001\n\ndata_loader = RSSCN7_DataLoader(data_dir=data_dir, batch_size=batch_size, shuffle=True)\n\npretrained_model_path = '/kaggle/input/resnet18-pretrained-on-dtd/pytorch/version1/1/resnet18_trained_on_DTD_from_80_to_90.pth'\npretrained_resnet18 = ResNet18()\npretrained_resnet18.load_state_dict(torch.load(pretrained_model_path, map_location=device))\n\npretrained_resnet18.fc = nn.Linear(pretrained_resnet18.resnet18.fc.in_features, 7)\n\npretrained_resnet18 = pretrained_resnet18.to(device)\n\nfor name, param in pretrained_resnet18.named_parameters():\n    if 'fc' not in name:\n        param.requires_grad = False\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(pretrained_resnet18.parameters(), lr=learning_rate)\n\n# most optimal\nepochs = 100\n\ndef train_model(model, criterion, optimizer, train_loader, test_loader, num_epochs, learning_rate):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct_predictions = 0\n        total_samples = 0\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            correct_predictions += (predicted == labels).sum().item()\n            total_samples += labels.size(0)\n        \n        knn_metric = np.mean(k_nearest_metric(5, model, train_loader, device))\n        epoch_loss = running_loss / total_samples\n        epoch_accuracy = correct_predictions / total_samples \n\n        if epoch_accuracy >= 0.85:\n            learning_rate = 0.0001\n\n\n        time_cur = time.time() - time0\n        print(\"learning_rate = \", learning_rate)\n\n        print(f'Training - Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n        print(f'KNN(k=5) metric train: {knn_metric:.4f}')\n        test_loss, test_accuracy, test_knn = evaluate_model(model, criterion,  test_loader)\n        print(f'Testing - Epoch {epoch+1}/{num_epochs}, Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}')\n        print(f'KNN(k=5) metric test: {test_knn:.4f}')\n\n        print(f'Time: {time_cur:.2f} seconds')\n\n        knn_metric_test.append(test_knn)\n        knn_metric_train.append(knn_metric)\n        train_acc.append(epoch_accuracy)\n        test_acc.append(test_accuracy)\n        loss_train.append(epoch_loss)\n        loss_test.append(test_loss)\n        learning_rates.append(learning_rate)\n        epoch_time.append(time_cur)\n\n    print('Training complete.')\n\ndef evaluate_model(model, criterion, test_loader):\n    model.eval()\n    running_loss = 0.0\n    correct_predictions = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            correct_predictions += (predicted == labels).sum().item()\n            total_samples += labels.size(0)\n\n    knn_metric = np.mean(k_nearest_metric(5, model, test_loader, device))\n    test_loss = running_loss / total_samples\n    test_accuracy = correct_predictions / total_samples\n\n    return test_loss, test_accuracy, knn_metric\n\ntrain_loader = data_loader.get_train_dataloader()\ntest_loader = data_loader.get_test_dataloader()\n\ntrain_model(pretrained_resnet18, criterion, optimizer, train_loader, test_loader, epochs, learning_rate)\n\nprint('Training completed successfully.')\nprint(\"KNN train:\")\nprint(knn_metric_train)\nprint(\"KNN test:\")\nprint(knn_metric_test)\nprint(\"Training accuracy:\")\nprint(train_acc)\nprint(\"Test accuracy:\")\nprint(test_acc)\nprint('Loss train:')\nprint(loss_train)\nprint(\"Test loss:\")\nprint(loss_test)\nprint(\"Learning rate:\")\nprint(learning_rates)\nprint(\"Epoch times:\")\nprint(epoch_time)\n\ntorch.save(pretrained_resnet18, 'resnet18_DTD_transfer_learning.pth')\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-01T14:50:54.347678Z",
     "iopub.execute_input": "2024-06-01T14:50:54.348162Z",
     "iopub.status.idle": "2024-06-01T15:54:56.135105Z",
     "shell.execute_reply.started": "2024-06-01T14:50:54.348127Z",
     "shell.execute_reply": "2024-06-01T15:54:56.134149Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
